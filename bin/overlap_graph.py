#!/usr/bin/env python3
# ==============================================================
# Lars Gabriel
#
# Graph for transcripts of multiple genome annotations.
# It can detect overlapping transcripts.
# Add a feature vector to each node.
# Compare nodes with the 'decision rule'.
# ==============================================================
from features import Node_features
import numpy as np
import tensorflow as tf

node_features_train = {'mean' : np.array([3.9197621255235475, 2.4418285172779752, 1.7003730863953093, 2.2430881275864505, 0.20668479418826924, 4.919666208395946, 2.957447362948991, 1.996771703917135, 2.6643059799624265, 0.24560893414018364, 1.3168142724685872, 0.5651913638762794, 0.5168957155011251, -2.7816753144497905, 0.44242169148047955, -3.8244212417209953, 0.39421189975635457, -4.601784487209027, 0.38006987732746306, -4.035099816931743, 0.0, -10.0, 0.44497873837004825, -5.350502693691524, 0.28627425904018927, -7.136455330829143, 0.348831409662052, -6.096595025121697, 0.0, -10.0, 0.4154298685935352, -5.616889118138075, 0.2987051187773763, -7.012014857015973, 0.3489848770662148, -6.07850620594717, 0.8912628626556002, 0.8518695740641473, 1.1076031530179236, 0.09097770648940776, 0.8686578289847322, 0.28233742745957796, 0.5135428551453172, 0.08474972153447004, 0.8761254681462372, 0.0024483705448554945, 0.0043461738108867215, 0.0008466533168574143, 0.8698265503190826, 0.6431374163545294, 0.6932849631916088, 0.018913559111494505, 0.8787356272765221, 0.9759562778986774, 1.2759862343571375, 0.10861515563825064, 0.20423953703999745, 0.5347188029542476, 0.21069156249000864, 0.21069156249000864, 0.3926433794237741, 0.4906879003424196, -4.39332406273994, 3.5358698084854687, 0.37879360388447714, 0.45999045981763936, 0.15874979943435077, 0.11009797332243985, 0.45939235523488914, 0.381111537660309, 0.46028956014046063, 0.15963991935196056, 0.10936486235105047, 0.4609930354809062, 0.3893425142144181, 0.4730892245556601, 0.16224183198904288, 0.1131178866706763, 0.46081048276739456, 0.7861751446750008]), 
                      'std' : np.array([4.937015563980883, 1.3394832163709156, 0.9344462799547661, 1.223296786721436, 0.21677201087355166, 4.936877357513892, 0.3400984664106627, 0.5788932299998389, 0.33295470253549847, 0.20528312211733474, 0.4652343379721678, 0.45459868172305884, 0.4614792081055513, 6.105399166207837, 0.44238767337387547, 5.694804736212401, 0.42487973485775876, 5.270808237762784, 0.41945232039147806, 5.947871830254574, 0.0, 0.0, 0.4969634400727464, 5.208550912289677, 0.45201914522689923, 4.521484839613401, 0.4766005217115324, 5.334972848705884, 0.0, 0.0, 0.4927959951883525, 5.216128712454615, 0.4576902563883938, 4.578365074458468, 0.47664917145091645, 5.35769820930469, 0.24573776470064246, 0.9786618140114467, 1.1191602604577817, 0.16587669711542094, 0.26984902198640587, 0.515152857391217, 0.5376424425355326, 0.17101172793732838, 0.2708523377544013, 0.027670839678085903, 0.03691472143454331, 0.010877091061643569, 0.2745534529199114, 0.6563267515948829, 0.6948299843313601, 0.05258785553735398, 0.2605772315601926, 1.009444454715118, 1.1196344355900758, 0.19717299857103196, 0.40314482329529766, 0.5443160638448301, 0.4077997400512828, 0.4077997400512828, 0.42425289139191524, 0.4882191062134801, 5.46805047508378, 5.022239149181895, 0.38814207448307375, 0.451766591458116, 0.2780585592949993, 0.1396276214223446, 0.46000640745499305, 0.3892180224885208, 0.452032041329858, 0.2784512363762027, 0.13872802969360098, 0.4611003022115861, 0.4042527125181476, 0.4771381293942819, 0.2861980117311413, 0.14756066516673963, 0.46093409073921904, 0.8502134289213223])}

node_features_train_altseqs = {'mean' : np.array([5.312417975453929, 2.821658391523538, 1.8369019643721003, 2.5640141418793077, 0.2682141338899816, 6.312241307200853, 3.015576138681299, 1.8286151851670869, 2.6515345778250485, 0.2848372561728252, 1.2245092949133964, 0.5791415419464937, 0.5182867497127192, -2.0686948557344316, 0.43296260010250576, -3.151692532389417, 0.38189285383498883, -4.030296950553245, 0.36807139147596685, -3.4056308672557054, 0.0, -10.0, 0.3543135897546835, -6.309715486095634, 0.22028728421234803, -7.796446352418534, 0.269422691414644, -6.986517116698709, 0.0, -10.0, 0.3490640188061553, -6.320017436976407, 0.24916352990380594, -7.50749285163123, 0.2922128960613796, -6.717040630309622, 0.8335801105430775, 0.8137802150475515, 1.1703918161182132, 0.12561676539422634, 0.8107689052574513, 0.2800193686304019, 0.5656296528910825, 0.1042303176375549, 0.8232143637006669, 0.003023850761932593, 0.006281914820594032, 0.001458473330036221, 0.8181903278601785, 0.7040513181922382, 0.7656669069645275, 0.02312614522013498, 0.808735180657964, 0.9580618345372925, 1.3917994832452087, 0.1554729065409239, 0.11609267511789902, 0.6088456712672522, 0.16771946523601436, 0.16771946523601436, 0.3777305277083349, 0.5290685076987858, -3.8315841434962343, 4.178413302758909, 0.38903640185618255, 0.5082660911700524, 0.11470355922388444, 0.14604256205840604, 0.4841113865773216, 0.39551653215586785, 0.509072142911013, 0.11605738513129733, 0.14451477579035824, 0.4883414138381367, 0.4063603930209194, 0.5280905576103185, 0.11843873254167979, 0.15033606662209495, 0.48804741065293267, 4.969468841488917]), 
                      'std' : np.array([5.672028546902703, 1.1499986870772918, 0.7471408794458769, 1.039517765204635, 0.2161210886698448, 5.671944514836058, 0.346464945562717, 0.5577103667049562, 0.33778411517374923, 0.19434498445973813, 0.417258758340134, 0.40862729991063995, 0.42570067136603457, 5.8980227579787625, 0.40634374583048627, 5.604535289992656, 0.39100749974458454, 5.236600920185983, 0.3858856441879945, 5.9205698227589245, 0.0, 0.0, 0.4783047876303248, 4.995573174653115, 0.41444034145605096, 4.145709049343844, 0.4436598976304945, 4.963875734536432, 0.0, 0.0, 0.4766742384273163, 5.039965158614276, 0.4325286872217256, 4.326831122185463, 0.45477963832755236, 5.110844329851363, 0.2798493671607951, 0.9256261896374527, 1.0907054962501141, 0.19907440116781913, 0.30349115715194686, 0.5372376229285164, 0.5378480625144142, 0.18734744071877807, 0.30285183361426704, 0.03072088948137231, 0.044368674316297076, 0.01430647890177635, 0.30487707418948845, 0.6505663601227105, 0.6920365002188631, 0.057706214147826625, 0.29735133653103163, 0.9586921512058494, 1.0593443453477658, 0.2334225914281703, 0.3203360202997383, 0.5765099044285492, 0.37361697795596616, 0.37361697795596616, 0.3893854180432258, 0.4790435700381513, 5.404419983329497, 5.3905495387892755, 0.3696133988451901, 0.4489331055505157, 0.24572063730430632, 0.1573443828355322, 0.4409794188943417, 0.37234119736935795, 0.44960137868072414, 0.24659999268312915, 0.1559047396761923, 0.44368716698425176, 0.3910779304359238, 0.48544383008700454, 0.25484052044642697, 0.1676617783993043, 0.44343539098257434, 5.83372985885662])}

edge_features_train_ni_nj = {'mean' : np.array([0.5105981487807656, 0.1314072844988857, 0.08221495615961479, 0.13573438745465488, 0.3101361573373676, 0.0529988775560002, 0.09067395441901323, 0.18391814292453598, 0.06027036259821385, 0.11258601337172418, 0.2275795877865079, 6.232264571437867, 0.5940006181575651, 0.2732907129959494, 0.07779024937777561, 0.09051128137556326, 0.3063621427293283, 0.05493468677305484, 0.06319847738031331, 0.21520016917996518, 0.0638003676410782, 0.07495973842174614, 0.25503879752086284, 0.34558261350511604, 7.293039220470776, 0.43316350250443725, 0.17791121273265345, 0.2238446802698824, 0.5793628627849285, -2.4547302576658203, -2.981204321894225, -2.7318996778679994, -3.8094527721577327, 0.9388685624083439, 0.5290506807806822, -3.169897290179528, -3.7366471419327167, -3.4506394861968523, -3.9203057515279562, 0.8992529281756578, 0.4664061053988406, -4.013210223249491, -4.324489352031312, -4.321934250496293, -4.3264669896690195, 0.8987131806680372, 0.4515414434594751, -3.3568152037220362, -3.7121790364239207, -3.6471330788347074, -4.394206590122336, 0.889729283413145, 0.6283952227201798, -1.8077477871005927, -2.376130862364053, -2.109823674323389, -3.3602762762472183, 0.9384875455739119, 0.3478997153840104, -5.238245174875161, -5.366158329448339, -5.280194616021183, -5.832254425315628, 0.8844189936940596, 0.1254463437883801, -8.24277483563816, -8.296767693658222, -8.265246362208906, -8.340151056876465, 0.9429507094233365, 0.07741900022817685, -8.875419222721566, -8.899442717573427, -8.898673433039564, -8.900633116514063, 0.959334864684456, 0.08155985221393065, -8.68936518880512, -8.718202819110196, -8.710881287116813, -8.853053764639679, 0.9579586836755958, 0.37429578277761255, -4.883194768824739, -5.027811631590307, -4.927817531762137, -5.525452525850302, 0.8751406820151629, 0.29974656123642673, -5.423194224203148, -5.588426532715968, -5.475424666893753, -6.004706289661709, 0.8434769918674873, 0.18877564744753716, -7.219331611848373, -7.323878307478357, -7.261825004134671, -7.391147716739447, 0.9004961115915754, 0.12283546724400966, -8.086800679182005, -8.132709363549736, -8.131346252444391, -8.134354138282312, 0.920934857892091, 0.12313820963281485, -7.8396380736314875, -7.893354282962285, -7.880028346113445, -8.115446565556896, 0.9180808381411533, 0.37050021143481965, -4.477061068170528, -4.688976700640151, -4.53909347258635, -5.172573471853717, 0.8191063563878773, 0.8751376803567726, 0.8016655123954558]), 
                      'std' : np.array([0.4998876666236357, 0.33784524575529923, 0.2746919313399159, 0.34250629704732827, 0.462549155495396, 0.2240312400846023, 0.2871448909681749, 0.3874174229791776, 0.23798707105760275, 0.3160860689192991, 0.4192697449251313, 121.45248130736982, 0.49108439578763513, 0.4456488518848111, 0.2678412337177946, 0.28691286014979017, 0.46098197386852796, 0.22785273086395344, 0.2433196043007711, 0.4109611372926446, 0.24439697365134666, 0.2633263679115462, 0.43588302247292743, 20.497307462469106, 6.843110061354367, 0.34504950728770334, 0.2076733673472856, 0.24087865193442193, 0.47650752009534414, 6.069720659761492, 5.640877050779364, 5.8360439887500615, 4.945855939275399, 0.17164653485838005, 0.45999289808916954, 5.62174559894281, 5.161244470993499, 5.379673216918491, 4.9909815206024275, 0.22699468363020794, 0.4436464863349523, 5.2372899789608764, 4.957386351654138, 4.9596969689418895, 4.955577706032927, 0.24187463338027207, 0.4389460671726021, 5.918921846475154, 5.596803284241396, 5.653016400497934, 4.98800571561147, 0.24705196706617635, 0.4671607022178574, 5.9860072484177795, 5.569956601966938, 5.753297013711885, 4.824348069893706, 0.17438035425681403, 0.4413440944788378, 5.703377414940464, 5.546736859425727, 5.649493238258892, 4.968996091901867, 0.2885401487572066, 0.3046136150069087, 3.9693892165152596, 3.8447543422620885, 3.9150058775564878, 3.7388261928746185, 0.2127178606961817, 0.24056130389518912, 3.2022854902723172, 3.132127146499215, 3.1343519692116417, 3.128671916147085, 0.1811383409011131, 0.24736429026023543, 3.649808480483717, 3.56760498239915, 3.587807734962767, 3.1913716067524693, 0.18366077585708093, 0.44729392367399273, 5.766385924469058, 5.600085746251449, 5.711714556640792, 5.0173771819512245, 0.297769748464872, 0.4050037419965848, 5.693526377877887, 5.483721592759384, 5.623768403082175, 4.947647687849924, 0.3274098140759008, 0.35076052794687695, 4.726535379519601, 4.545890544386357, 4.649144624912796, 4.421222439878237, 0.2704957811477945, 0.2856174416462625, 3.9983488045233906, 3.899907844490781, 3.902806030074006, 3.8963957659763655, 0.24614066255537734, 0.285382132569038, 4.495496399595772, 4.381036284746695, 4.408484770219632, 3.9179355385906596, 0.2497089065164395, 0.42609753296976216, 5.8231482006428585, 5.596170582863224, 5.751299475614854, 5.06256929608878, 0.34469220753193275, 0.20169877054666321, 0.2419723387956009])}

edge_features_train_ni_nj_altseqs = {'mean' :np.array([0.48718432401372114, 0.1226790450928382, 0.06599521733830947, 0.07800846252517572, 0.19877647441217955, 0.041369443202990325, 0.049488069464678534, 0.10906384040768709, 0.048058784948545756, 0.06202079064827122, 0.1366076766363494, 4.65890016193721, 0.6209871204778018, 0.2877650343318494, 0.06653210593827813, 0.04799058558584703, 0.21774605459431537, 0.047202665289136275, 0.0341867443655718, 0.15235012101759254, 0.05448693764546779, 0.03993870763658736, 0.17909152644684223, 0.3155642554080644, 10.544242522737958, 0.5314604731510224, 0.18418715039541497, 0.2582175100369395, 0.5898456132674232, -1.4764659507845743, -2.2094972604041883, -1.7922431719669323, -2.8264957856169883, 0.8444605283480376, 0.5096836401893284, -2.197852997007582, -2.88300049521884, -2.5151645776783593, -3.0183548295553835, 0.7934660172566048, 0.4373429252673282, -3.2428920202157965, -3.5956346948719853, -3.5898048873512614, -3.5972815221483345, 0.8038251690594376, 0.42230466621412305, -2.5205442056465936, -2.9245108337247445, -2.8439202485874167, -3.681765272801311, 0.7972260492629459, 0.6894232495099949, -0.07161197602325822, -0.9589975712413913, -0.44089426609080384, -1.7464291963840282, 0.8206996379951794, 0.19065243854905833, -6.8169298476910996, -6.923976169948004, -6.852323501346249, -7.180852496333755, 0.8623505422433867, 0.0950082402072292, -8.344192600996973, -8.40041268248742, -8.367457360902678, -8.450881652276424, 0.9167303087911519, 0.06686305610179721, -8.82844527636999, -8.853998187791133, -8.853120413200541, -8.855250724388801, 0.9387240272096647, 0.06485555162157466, -8.6999439381733, -8.728632181753415, -8.722058842627485, -8.868432339050546, 0.937452007457121, 0.2284185966300456, -6.178342428733754, -6.314153876265951, -6.219486105532651, -6.6237548771804065, 0.8326218648150252, 0.19248750161913528, -6.2638237440255935, -6.439704114333773, -6.319088497223144, -6.715145666711682, 0.8107537490892267, 0.12332142547208362, -7.524150151756585, -7.637433500941279, -7.571084435993277, -7.7034072816942425, 0.8657925245843109, 0.08983751710575084, -8.151171079522884, -8.202742740715403, -8.200914160240888, -8.204612926004007, 0.8947250759159698, 0.0871947067296797, -7.948128210651916, -8.007564176260269, -7.993394480574365, -8.22405977084527, 0.8936114281023968, 0.2457132214513939, -5.282197221400834, -5.513522953198335, -5.350760318532476, -5.864620689588724, 0.7629210768407989, 0.892625801415113, 0.8275901017831679]) , 
                      'std' : np.array([0.4998357314667132, 0.3280684333915016, 0.24827373728754248, 0.2681849031905886, 0.3990794252169158, 0.19914319564714927, 0.21688476305486393, 0.31171929539745435, 0.21389048164295016, 0.24119330872823522, 0.34343269984427, 59.369094704805775, 0.48514133680648686, 0.45272101712553003, 0.24920992118572635, 0.2137463199252126, 0.4127138358512813, 0.21207209547237382, 0.18170858778688018, 0.3593599332761423, 0.22697601475078077, 0.19581523757949743, 0.38342893944139633, 16.165251714240934, 7.427395463011521, 0.27951808625768787, 0.17480185642875523, 0.20447540716810866, 0.42817151770822465, 5.60420178059581, 5.1120492387538246, 5.381927240613792, 4.668140409300158, 0.2602318110896353, 0.4050057192541004, 5.3173869263795135, 4.858884069138346, 5.08744309838286, 4.740792187101532, 0.3066825893045647, 0.3923309089463862, 5.077082374022575, 4.803301694974266, 4.80785871576613, 4.801968733367379, 0.3130172884105072, 0.3871713554107427, 5.7565771477101695, 5.4392816708806615, 5.499223587054206, 4.854885923120252, 0.313621703947183, 0.38528696448976, 4.9189061494729485, 4.485077072432246, 4.7160018730617255, 4.043353884262707, 0.27738416598325666, 0.3381714407252723, 5.138829632383853, 4.961476527627555, 5.078059258626704, 4.533787724409752, 0.3163972894071392, 0.24894101997828993, 3.8978444674273756, 3.764040826463332, 3.8398608562233507, 3.63772876847893, 0.2572616055272157, 0.20862152631962544, 3.2609495364699006, 3.1880746013095314, 3.1905564180448844, 3.18452154403465, 0.22298025687225206, 0.20466767943903214, 3.6470654861751033, 3.564931983483261, 3.583174000234811, 3.172218540994504, 0.2255720882246783, 0.3585551866997077, 5.425111528200914, 5.228914360762155, 5.362324838181835, 4.773286323707506, 0.3425340558149359, 0.3132199374479994, 5.426172127727807, 5.164184875729916, 5.3405055227405365, 4.750005466666285, 0.3535463628504991, 0.2598911868183844, 4.5821895685557, 4.371041467910501, 4.490477225817602, 4.239372099878305, 0.30956369566779895, 0.22066629523622164, 3.956094395849598, 3.8431935508506814, 3.8471693051018607, 3.839104301330607, 0.278988362321698, 0.21622740538439716, 4.427980812459606, 4.297387108581756, 4.327572735732354, 3.8294770874289927, 0.2804607427474543, 0.33798579785743005, 5.725290499521565, 5.440250371314883, 5.635241937754491, 4.995177090889143, 0.38112628894499617, 0.17396963266306043, 0.217339498012742])}

edge_features_train_nj_ni = {'mean' : np.array([0.5105981487807656, 0.3579782994160038, 0.13573438745465488, 0.08221495615961479, 0.3101361573373676, 0.09067395441901323, 0.0529988775560002, 0.18391814292453598, 0.11258601337172418, 0.06027036259821385, 0.2275795877865079, -6.232264571437867, 0.5940006181575651, 0.13270866884648544, 0.09051128137556326, 0.07779024937777561, 0.3063621427293283, 0.06319847738031331, 0.05493468677305484, 0.21520016917996518, 0.07495973842174614, 0.0638003676410782, 0.25503879752086284, -0.34558261350511604, 7.293039220470776, 0.43316350250443725, 0.2238446802698824, 0.17791121273265345, 0.5793628627849285, -2.4547302576658203, -2.981204321894225, -2.7318996778679994, -3.8094527721577327, 0.9388685624083439, 0.5290506807806822, -3.169897290179528, -3.7366471419327167, -3.4506394861968523, -3.9203057515279562, 0.8992529281756578, 0.4664061053988406, -4.013210223249491, -4.324489352031312, -4.321934250496293, -4.3264669896690195, 0.8987131806680372, 0.4515414434594751, -3.3568152037220362, -3.7121790364239207, -3.6471330788347074, -4.394206590122336, 0.889729283413145, 0.6283952227201798, -1.8077477871005927, -2.376130862364053, -2.109823674323389, -3.3602762762472183, 0.9384875455739119, 0.29974656123642673, -5.423194224203148, -5.588426532715968, -5.475424666893753, -6.004706289661709, 0.8434769918674873, 0.18877564744753716, -7.219331611848373, -7.323878307478357, -7.261825004134671, -7.391147716739447, 0.9004961115915754, 0.12283546724400966, -8.086800679182005, -8.132709363549736, -8.131346252444391, -8.134354138282312, 0.920934857892091, 0.12313820963281485, -7.8396380736314875, -7.893354282962285, -7.880028346113445, -8.115446565556896, 0.9180808381411533, 0.37050021143481965, -4.477061068170528, -4.688976700640151, -4.53909347258635, -5.172573471853717, 0.8191063563878773, 0.3478997153840104, -5.238245174875161, -5.366158329448339, -5.280194616021183, -5.832254425315628, 0.8844189936940596, 0.1254463437883801, -8.24277483563816, -8.296767693658222, -8.265246362208906, -8.340151056876465, 0.9429507094233365, 0.07741900022817685, -8.875419222721566, -8.899442717573427, -8.898673433039564, -8.900633116514063, 0.959334864684456, 0.08155985221393065, -8.68936518880512, -8.718202819110196, -8.710881287116813, -8.853053764639679, 0.9579586836755958, 0.37429578277761255, -4.883194768824739, -5.027811631590307, -4.927817531762137, -5.525452525850302, 0.8751406820151629, 0.8016655123954558, 0.8751376803567726]), 
                      'std' : np.array([0.4998876666236357, 0.479405711859105, 0.34250629704732827, 0.2746919313399159, 0.462549155495396, 0.2871448909681749, 0.2240312400846023, 0.3874174229791776, 0.3160860689192991, 0.23798707105760275, 0.4192697449251313, 121.45248130736982, 0.49108439578763513, 0.33925960275208833, 0.28691286014979017, 0.2678412337177946, 0.46098197386852796, 0.2433196043007711, 0.22785273086395344, 0.4109611372926446, 0.2633263679115462, 0.24439697365134666, 0.43588302247292743, 20.497307462469106, 6.843110061354367, 0.34504950728770334, 0.24087865193442193, 0.2076733673472856, 0.47650752009534414, 6.069720659761492, 5.640877050779364, 5.8360439887500615, 4.945855939275399, 0.17164653485838005, 0.45999289808916954, 5.62174559894281, 5.161244470993499, 5.379673216918491, 4.9909815206024275, 0.22699468363020794, 0.4436464863349523, 5.2372899789608764, 4.957386351654138, 4.9596969689418895, 4.955577706032927, 0.24187463338027207, 0.4389460671726021, 5.918921846475154, 5.596803284241396, 5.653016400497934, 4.98800571561147, 0.24705196706617635, 0.4671607022178574, 5.9860072484177795, 5.569956601966938, 5.753297013711885, 4.824348069893706, 0.17438035425681403, 0.4050037419965848, 5.693526377877887, 5.483721592759384, 5.623768403082175, 4.947647687849924, 0.3274098140759008, 0.35076052794687695, 4.726535379519601, 4.545890544386357, 4.649144624912796, 4.421222439878237, 0.2704957811477945, 0.2856174416462625, 3.9983488045233906, 3.899907844490781, 3.902806030074006, 3.8963957659763655, 0.24614066255537734, 0.285382132569038, 4.495496399595772, 4.381036284746695, 4.408484770219632, 3.9179355385906596, 0.2497089065164395, 0.42609753296976216, 5.8231482006428585, 5.596170582863224, 5.751299475614854, 5.06256929608878, 0.34469220753193275, 0.4413440944788378, 5.703377414940464, 5.546736859425727, 5.649493238258892, 4.968996091901867, 0.2885401487572066, 0.3046136150069087, 3.9693892165152596, 3.8447543422620885, 3.9150058775564878, 3.7388261928746185, 0.2127178606961817, 0.24056130389518912, 3.2022854902723172, 3.132127146499215, 3.1343519692116417, 3.128671916147085, 0.1811383409011131, 0.24736429026023543, 3.649808480483717, 3.56760498239915, 3.587807734962767, 3.1913716067524693, 0.18366077585708093, 0.44729392367399273, 5.766385924469058, 5.600085746251449, 5.711714556640792, 5.0173771819512245, 0.297769748464872, 0.2419723387956009, 0.20169877054666321])}

edge_features_train_nj_ni_altseqs = {'mean' :np.array([0.48718432401372114, 0.3900901972847947, 0.07800846252517572, 0.06599521733830947, 0.19877647441217955, 0.049488069464678534, 0.041369443202990325, 0.10906384040768709, 0.06202079064827122, 0.048058784948545756, 0.1366076766363494, -4.65890016193721, 0.6209871204778018, 0.09124784519034877, 0.04799058558584703, 0.06653210593827813, 0.21774605459431537, 0.0341867443655718, 0.047202665289136275, 0.15235012101759254, 0.03993870763658736, 0.05448693764546779, 0.17909152644684223, -0.3155642554080644, 10.544242522737958, 0.5314604731510224, 0.2582175100369395, 0.18418715039541497, 0.5898456132674232, -1.4764659507845743, -2.2094972604041883, -1.7922431719669323, -2.8264957856169883, 0.8444605283480376, 0.5096836401893284, -2.197852997007582, -2.88300049521884, -2.5151645776783593, -3.0183548295553835, 0.7934660172566048, 0.4373429252673282, -3.2428920202157965, -3.5956346948719853, -3.5898048873512614, -3.5972815221483345, 0.8038251690594376, 0.42230466621412305, -2.5205442056465936, -2.9245108337247445, -2.8439202485874167, -3.681765272801311, 0.7972260492629459, 0.6894232495099949, -0.07161197602325822, -0.9589975712413913, -0.44089426609080384, -1.7464291963840282, 0.8206996379951794, 0.19248750161913528, -6.2638237440255935, -6.439704114333773, -6.319088497223144, -6.715145666711682, 0.8107537490892267, 0.12332142547208362, -7.524150151756585, -7.637433500941279, -7.571084435993277, -7.7034072816942425, 0.8657925245843109, 0.08983751710575084, -8.151171079522884, -8.202742740715403, -8.200914160240888, -8.204612926004007, 0.8947250759159698, 0.0871947067296797, -7.948128210651916, -8.007564176260269, -7.993394480574365, -8.22405977084527, 0.8936114281023968, 0.2457132214513939, -5.282197221400834, -5.513522953198335, -5.350760318532476, -5.864620689588724, 0.7629210768407989, 0.19065243854905833, -6.8169298476910996, -6.923976169948004, -6.852323501346249, -7.180852496333755, 0.8623505422433867, 0.0950082402072292, -8.344192600996973, -8.40041268248742, -8.367457360902678, -8.450881652276424, 0.9167303087911519, 0.06686305610179721, -8.82844527636999, -8.853998187791133, -8.853120413200541, -8.855250724388801, 0.9387240272096647, 0.06485555162157466, -8.6999439381733, -8.728632181753415, -8.722058842627485, -8.868432339050546, 0.937452007457121, 0.2284185966300456, -6.178342428733754, -6.314153876265951, -6.219486105532651, -6.6237548771804065, 0.8326218648150252, 0.8275901017831679, 0.892625801415113]) , 
                      'std' : np.array([0.4998357314667132, 0.48777026894528686, 0.2681849031905886, 0.24827373728754248, 0.3990794252169158, 0.21688476305486393, 0.19914319564714927, 0.31171929539745435, 0.24119330872823522, 0.21389048164295016, 0.34343269984427, 59.369094704805775, 0.48514133680648686, 0.2879612403388697, 0.2137463199252126, 0.24920992118572635, 0.4127138358512813, 0.18170858778688018, 0.21207209547237382, 0.3593599332761423, 0.19581523757949743, 0.22697601475078077, 0.38342893944139633, 16.165251714240934, 7.427395463011521, 0.27951808625768787, 0.20447540716810866, 0.17480185642875523, 0.42817151770822465, 5.60420178059581, 5.1120492387538246, 5.381927240613792, 4.668140409300158, 0.2602318110896353, 0.4050057192541004, 5.3173869263795135, 4.858884069138346, 5.08744309838286, 4.740792187101532, 0.3066825893045647, 0.3923309089463862, 5.077082374022575, 4.803301694974266, 4.80785871576613, 4.801968733367379, 0.3130172884105072, 0.3871713554107427, 5.7565771477101695, 5.4392816708806615, 5.499223587054206, 4.854885923120252, 0.313621703947183, 0.38528696448976, 4.9189061494729485, 4.485077072432246, 4.7160018730617255, 4.043353884262707, 0.27738416598325666, 0.3132199374479994, 5.426172127727807, 5.164184875729916, 5.3405055227405365, 4.750005466666285, 0.3535463628504991, 0.2598911868183844, 4.5821895685557, 4.371041467910501, 4.490477225817602, 4.239372099878305, 0.30956369566779895, 0.22066629523622164, 3.956094395849598, 3.8431935508506814, 3.8471693051018607, 3.839104301330607, 0.278988362321698, 0.21622740538439716, 4.427980812459606, 4.297387108581756, 4.327572735732354, 3.8294770874289927, 0.2804607427474543, 0.33798579785743005, 5.725290499521565, 5.440250371314883, 5.635241937754491, 4.995177090889143, 0.38112628894499617, 0.3381714407252723, 5.138829632383853, 4.961476527627555, 5.078059258626704, 4.533787724409752, 0.3163972894071392, 0.24894101997828993, 3.8978444674273756, 3.764040826463332, 3.8398608562233507, 3.63772876847893, 0.2572616055272157, 0.20862152631962544, 3.2609495364699006, 3.1880746013095314, 3.1905564180448844, 3.18452154403465, 0.22298025687225206, 0.20466767943903214, 3.6470654861751033, 3.564931983483261, 3.583174000234811, 3.172218540994504, 0.2255720882246783, 0.3585551866997077, 5.425111528200914, 5.228914360762155, 5.362324838181835, 4.773286323707506, 0.3425340558149359, 0.217339498012742, 0.17396963266306043])}

class BatchSizeError(Exception):
    pass

class Edge:
    """
        Class handling an edge in the overlap graph.
    """
    def __init__(self, n1_id, n2_id):
        """
            Args:
                n1_id (str): Node ID from overlap graph
                n2_id (str): Node ID from overlap graph
        """
        self.node1 = n1_id
        self.node2 = n2_id
        self.epsi = 1e-10
        self.node_to_remove = None
        self.__numb_features__ = 240

        ### feature vectors for directed edge from node n_i to node n_j
        ### features:

        # numb introns in union
         #### For src in E, P, C and M, all:
            # number of introns in n_i / numb introns in union of n_i, n_j
            # number of introns in n_j / numb introns in union of n_i, n_j !
            # number of introns in n_i and in n_j / numb introns in union of n_i, n_j
        # fraction of CDS of n_i that is also in n_j
        # fraction of CDS of n_j that is also in n_i !
        # start codon position difference of nj to ni if they agree on first coding DSS
        # stop codon position difference of nj to ni if they agree on last coding ASS

        self.feature_vector_n1_to_n2 = np.zeros(self.__numb_features__, float)
        self.feature_vector_n2_to_n1 = np.zeros(self.__numb_features__, float)

    def add_features(self, tx1, tx2, n1, n2, evi):
        k=0
        ### for codon in start/stop_codon
            # 1, if codon of ni and nj matches
            # 1, if n_i is starting upstream(start_codon) or ending downstream(stop_codon)
            # for src in P, C, M
                # 1, if codon of ni is supported and not of nj
                # 1, if codon of nj is supported and not of ni
                # 1, if codon of nj is supported and also of ni
            # codon position difference of nj to ni if they agree on first/last coding DSS/ASS
        index = [0,-1]
        if tx1.strand == '-':
            index = [-1, 0]
        for i, type in enumerate(['start_codon', 'stop_codon']):
            if n1.coords[type] == n2.coords[type]:
                self.feature_vector_n1_to_n2[k] = 1.0
                self.feature_vector_n2_to_n1[k] = 1.0
            elif n1.coords[type] and n2.coords[type]:
                if (tx1.strand == '+' and \
                    n1.coords[type][0][0] < n2.coords[type][0][0]) \
                    or (tx1.strand == '-' and \
                    n1.coords[type][0][0] > n2.coords[type][0][0]):
                    self.feature_vector_n1_to_n2[k+1] = 1.0
                else :
                    self.feature_vector_n2_to_n1[k+1] = 1.0
            k+=2
            for src in ['P', 'C', 'M']:
                if src in n1.hints_sorted_by_src[type] and \
                    src not in n2.hints_sorted_by_src[type]:
                    self.feature_vector_n1_to_n2[k] = 1.0
                    self.feature_vector_n2_to_n1[k+1] = 1.0
                elif src in n2.hints_sorted_by_src[type] and \
                    src not in n1.hints_sorted_by_src[type]:
                    self.feature_vector_n1_to_n2[k+1] = 1.0
                    self.feature_vector_n2_to_n1[k] = 1.0
                elif src in n2.hints_sorted_by_src[type] and \
                    src in n1.hints_sorted_by_src[type]:
                    self.feature_vector_n1_to_n2[k+2] = 1.0
                    self.feature_vector_n2_to_n1[k+2] = 1.0
                k+=3    
            t = index[i]
            j = abs(t)
            if n1.coords['CDS'][t][t+1] == n2.coords['CDS'][t][t+1]:
                self.feature_vector_n1_to_n2[k] = n2.coords['CDS'][t][j] - n1.coords['CDS'][t][j]
                self.feature_vector_n2_to_n1[k] = n1.coords['CDS'][t][j] - n2.coords['CDS'][t][j]
            k += 1

        set_tx1 = set([f'{i[0]}_{i[1]}' for i in n1.coords['intron']])
        set_tx2 = set([f'{i[0]}_{i[1]}' for i in n2.coords['intron']])
        union_size = len(set_tx1.union(set_tx2))
        
        # numb introns in union
        # numb introns in intersection / numb introns in union
        # numb introns only in n_i / numb introns in union
        # numb introns only in n_j / numb introns in union
        self.feature_vector_n1_to_n2[k] = self.feature_vector_n2_to_n1[k] = union_size
        if len(n1.coords['intron']) > 0 and len(n2.coords['intron']) > 0:
            self.feature_vector_n1_to_n2[k+1] = self.feature_vector_n2_to_n1[k+1] = \
                len(set_tx1.intersection(set_tx2)) / union_size
            self.feature_vector_n1_to_n2[k+2] = self.feature_vector_n2_to_n1[k+3] \
                = len(set_tx1.difference(set_tx2)) / union_size
            self.feature_vector_n1_to_n2[k+3] = self.feature_vector_n2_to_n1[k+2] \
                =  len(set_tx2.difference(set_tx1)) / union_size
        k += 4


        ### for introns in intersection, in tx1 and not in tx2, in tx2 and not in tx1:
            ### for src in E, P, C, M, and any:
                # rel support of introns
                # abs support of introns (log10)
                # intron with min support (log10)
                # intron with max support (log10)
                # std of intron support (log10)
                # entropy of intron support (log10)
        intron_sets = [set_tx1.intersection(set_tx2), set_tx1.difference(set_tx2),
            set_tx2.difference(set_tx1)]
        for i_set, hints in zip(intron_sets, [n1.hints, n1.hints, n2.hints]):
            for evi_src in ['E', 'P', 'C', 'M', 'all']:                
                if evi_src == 'all':
                    current_hints = [sum([hints['intron'][s][e] for e in hints['intron'][s]]) \
                                     for s in i_set if s in hints['intron']]
                else:
                    current_hints = [hints['intron'][s][evi_src] for s in i_set if s in hints['intron'] \
                                    and evi_src in hints['intron'][s]]                
                if current_hints:
                    self.feature_vector_n1_to_n2[k] = len(current_hints) / len(i_set)
                    self.feature_vector_n1_to_n2[k+1] = np.log10(np.sum(current_hints))
                    self.feature_vector_n1_to_n2[k+2] = np.log10(np.min(current_hints))
                    self.feature_vector_n1_to_n2[k+3] = np.log10(np.max(current_hints))
                    self.feature_vector_n1_to_n2[k+4] = np.std(np.log10(current_hints))
                    if not len(i_set) == 1:
                        p = np.array(current_hints)/np.sum(current_hints)
                        self.feature_vector_n1_to_n2[k+5] = -np.sum(p * np.log10(p) / np.log10(len(i_set)))
                    else: 
                        self.feature_vector_n1_to_n2[k+5] = 1.0
                else:
                    self.feature_vector_n1_to_n2[k+1:k+5] = np.log10(self.epsi)
                    self.feature_vector_n1_to_n2[k+5] = 1.0
                
                k+=6
        self.feature_vector_n2_to_n1[k-90 : k-60] = self.feature_vector_n1_to_n2[k-90 : k-60]
        self.feature_vector_n2_to_n1[k-60 : k-30] = self.feature_vector_n1_to_n2[k-30 : k]
        self.feature_vector_n2_to_n1[k-30 : k] = self.feature_vector_n1_to_n2[k-60 : k-30]

        i = 0
        j = 0
        overlap_size = 0
        while i<len(n1.coords['CDS']) and j<len(n2.coords['CDS']):
            overlap_size += max(0, min(n1.coords['CDS'][i][1], \
                n2.coords['CDS'][j][1]) - max(n1.coords['CDS'][i][0], \
                    n2.coords['CDS'][j][0]) + 1)
            if n1.coords['CDS'][i][1] < n2.coords['CDS'][j][1]:
                i += 1
            else:
                j += 1
        # fraction of CDS of n_i that is also in n_j
        # fraction of CDS of n_j that is also in n_i
        self.feature_vector_n1_to_n2[k] = overlap_size / sum([c[1]-c[0]+1 \
            for c in n1.coords['CDS']])
        self.feature_vector_n1_to_n2[k+1] = overlap_size / sum([c[1]-c[0]+1 \
            for c in n2.coords['CDS']])
        self.feature_vector_n2_to_n1[k+1] = self.feature_vector_n1_to_n2[k]
        self.feature_vector_n2_to_n1[k] = self.feature_vector_n1_to_n2[k+1]
        #self.feature_vector_n2_to_n1[k] = overlap_size / sum([c[1]-c[0]+1 \
            #for c in coords_tx2['CDS']])

        k += 2        
        self.feature_vector_n1_to_n2[k:] = self.feature_vector_n1_to_n2[:k]
        self.feature_vector_n2_to_n1[k:] = self.feature_vector_n2_to_n1[:k]

class Node:
    """
        Class handling a node that represents a transcript in the overlap graph.
    """
    def __init__(self, a_id, t_id):
        """
            Args:
                a_id (str): Annotation ID of the transcript from Anno object
                t_id (str): Transcript ID from Transcrpt object
        """
        self.id = '{};{}'.format(a_id, t_id)
        self.transcript_id = t_id
        # ID of original annotation/gene prediction
        self.anno_id = a_id
        # unique ID for a cluster of overlapping transcripts
        self.component_id = None

        # dict of edge_ids of edges that are incident
        # self.edge_to[id of incident Node] = edge_id
        self.edge_to = {}
        # self.is_in_ref_anno[cdskey or 'is_in_ref_anno'] = 1 if in ref, 0 else
        self.is_in_ref_anno = {}
        self.ref_anno_cds_acc = 0.
        
        
        # feature vector
        # features in order:
        ### for type in "intron", "CDS", "3'-UTR", "5'-UTR":
            # numb of exons with type or if type == intron: number of introns
            # total len (in bp) of type
            # min len of type
            # max len of type
        #### For type in intron, start, stop:
            #### For src in E, P C and M:
                # rel intron hint support for src
            #### For src in E, P C and M:
                # abs intron hint support for src
        ### relative fraction of introns supported by any hint source
        # single exon tx?
        # for type in intron, CDS
            # max (numb. of type matching a protein chain / numb. of type in chain)
            # max (numb. of type matching a protein chain / numb. of type in tx)
        self.__numb_features__ = 160#39#46
        
        self.epsi = 1e-10
        self.feature_vector = np.zeros(self.__numb_features__, float)
        self.dup_sources = {}
        self.evi_support = False
        # self.hints[type][startcoord_endcoord][src] = hint_multiplicity
        self.hints = {}
        
        # self.hints_sorted_by_src[type][src] = [log10(hint_multiplicity)]
        self.hints_sorted_by_src = {}
        
        # self.coords[type] = [[starcoord, endcoord]]
        # self.cdspart_hints[groupID][cds_index] = [[overlap_start, overlap_end]]
        self.cdspart_hints = {}        
        self.coords = {}
        self.best_chain = ''
        # list of group IDs of matching protein chains
        self.matching_chains = []

    def add_evidence(self, tx, evi):
        for type in ["intron", "CDS", 'start_codon', 'stop_codon']:
            self.coords.update({type : tx.get_type_coords(type, frame=False)})
        for type in ['intron', 'start_codon', 'stop_codon']:
            if type not in self.hints: self.hints[type] = {}
            if type not in self.hints_sorted_by_src: self.hints_sorted_by_src[type] = {}
            for line in tx.transcript_lines[type]:
                hint = evi.get_hint(line[0], line[3], line[4], line[2], \
                    line[6])
                if hint:
                    all = 0
                    coord_key = f'{line[3]}_{line[4]}'
                    self.hints[type][coord_key] = hint
                    all = 0
                    for key in hint:
                        if key not in self.hints_sorted_by_src[type]:
                            self.hints_sorted_by_src[type][key] = []
                        self.hints_sorted_by_src[type][key].append(hint[key])
                        all +=hint[key]
                    if type == 'intron':
                        if not 'all' in self.hints_sorted_by_src[type]:
                            self.hints_sorted_by_src[type]['all'] = []
                        self.hints_sorted_by_src[type]['all'].append(all)

        self.matching_chains = evi.get_matching_chains(tx.chr, self.coords, tx.strand)
        intron_keys = [f"{tx.chr}_{c[0]}_{c[1]}_intron_{tx.strand}"
            for c in self.coords['intron']]
        start_stop_keys = [f"{tx.chr}_{c[0]}_{c[1]}_start_{tx.strand}"
            for c in self.coords['start_codon']]
        start_stop_keys += [f"{tx.chr}_{c[0]}_{c[1]}_stop_{tx.strand}"
            for c in self.coords['stop_codon']]
        self.best_chain = ''
        best_chain_match = 0
        best_border_match = 0
        for g in self.matching_chains:
            chain_match = len(set(evi.group_chains[g]['intron']).intersection(intron_keys))
            border_match = len(set(evi.group_chains[g]['start'] +
                evi.group_chains[g]['stop']).intersection(start_stop_keys))
            if chain_match < best_chain_match:
                continue
            elif chain_match == best_chain_match and border_match < best_border_match:
                continue
            best_chain_match = chain_match
            best_border_match = border_match
            self.best_chain = g
        
        for g in self.matching_chains:
            chain = sorted(evi.group_chains[g]['CDSpart'])
            chain_match = 0
            j=0
            self.cdspart_hints[g] = []
            for k, c1 in enumerate(self.coords['CDS']):
                self.cdspart_hints[g].append([])
                for i in range(j, len(chain)):
                    if chain[i][0] > c1[1]:
                        j = i
                        break   
                    if chain[i][1] < c1[0]:
                        continue
                    self.cdspart_hints[g][k].append([max(chain[i][0], c1[0]), min(chain[i][1], c1[1])])

    def add_features(self, tx, evi):
        """
            Compute for all nodes the feature vector based on the evidence support by evi.

            Args:
                evi (Evidence): Evidence class object with all hints from any source.
        """
        #coords = {}
        k = 0
        ### for type in "intron", "CDS":
            # numb of exons with type or if type == intron: number of introns
            # total len (in bp) of type
            # min len of type
            # max len of type
            # std of lengths of type
        for i, type in enumerate(["intron", "CDS"]):#, "3'-UTR", "5'-UTR"]):
            #coords.update({type : tx.get_type_coords(type, frame=False)})
            self.feature_vector[k] = 1.0 * len(self.coords[type])
            if self.feature_vector[k] > 0:
                len_type = [c[1]-c[0]+1 for c in self.coords[type]]
                self.feature_vector[k+1] = np.log10(np.sum(len_type))
                self.feature_vector[k+2] = np.log10(np.min(len_type))
                self.feature_vector[k+3] = np.log10(np.max(len_type))
                self.feature_vector[k+4] = np.std(np.log10(len_type))
            k+=5

        """
        # CDS predicted by BRAKER1?
        # CDS predicted by BRAKER2?
        # CDS predicted by long-read protocol?
        if 'anno1' in self.dup_sources:
            self.feature_vector[16] = 1.0
        if 'anno2' in self.dup_sources:
            self.feature_vector[17] = 1.0
        if 'anno3' in self.dup_sources:
            self.feature_vector[18] = 1.0"""

        # numb. input gene sets that include tx
        self.feature_vector[k] =  len(self.dup_sources)
        k+=1

        ### relative fraction of introns supported by any hint source
        if len(self.coords['intron']) > 0:
            self.feature_vector[k] = len(self.hints['intron'])/len(self.coords['intron'])
        k += 1

        #### For type in intron, start, stop:
            #### For src in E, P C and M:
                # rel type hint support for src
                # abs type hint support for src
        for type, abs_numb in zip(['intron', 'start_codon', 'stop_codon'], \
            [len(self.coords['intron']), 1, 1]) :
            for evi_src in ['E', 'P', 'C', 'M']:
                if abs_numb > 0 and evi_src in self.hints_sorted_by_src[type]:
                    self.feature_vector[k] = \
                        1.0*len(self.hints_sorted_by_src[type][evi_src])/abs_numb
                    self.feature_vector[k+1] = \
                        np.log10(np.sum(self.hints_sorted_by_src[type][evi_src]))
                else:
                    self.feature_vector[k+1] = np.log10(self.epsi)
                k+=2

        #### For src in E, P, C, M, and any:
            # Entropy H for intron hint of source src
            # min absolute type hint support for src
            # max absolute type hint support for src
            # std absolute type hint support for src
        for evi_src in ['E', 'P', 'C', 'M', 'all']:
            if len(self.coords['intron']) > 0 and evi_src in self.hints_sorted_by_src['intron']:
                current_hints = np.array(self.hints_sorted_by_src['intron'][evi_src])
                p = (current_hints/np.sum(current_hints)) + self.epsi
                if len(self.coords['intron']) == 1:
                    self.feature_vector[k] = 1.0
                else:
                    self.feature_vector[k] = - np.sum(p * np.log10(p) / np.log10(len(self.coords['intron'])))
                self.feature_vector[k+1] = np.log10(np.min(current_hints))
                self.feature_vector[k+2] = np.log10(np.max(current_hints))
                self.feature_vector[k+3] = np.std(np.log10(current_hints))
            else:
                self.feature_vector[k] = 1.0
            k+=4

        # 1, if tx is intronless
        if len(self.coords['intron']) == 0:
            self.feature_vector[k] = 1.0
        k += 1

        # number of matching_chains
        self.feature_vector[k] = len(self.matching_chains)
        k += 1

        ### start/stop_codon match best_chain
        for s in ['start', 'stop']:
            if self.best_chain and [f"{tx.chr}_{c[0]}_{c[1]}_start_{tx.strand}"
                for c in self.coords['start_codon']] == \
                evi.group_chains[self.best_chain]['start']:
                self.feature_vector[k] = 1.0
            k+=1

        # number of introns matching best_chain / number of introns in tx
        # number of introns matching best_chain / number of introns in chain
        if self.best_chain:
            intron_keys = [f"{tx.chr}_{c[0]}_{c[1]}_intron_{tx.strand}"
                for c in self.coords['intron']]
            chain_match = len(set(evi.group_chains[self.best_chain]['intron']).intersection(intron_keys))
            self.feature_vector[k] = chain_match / len(self.coords['intron'])
            self.feature_vector[k+1] = chain_match / len(evi.group_chains[self.best_chain]['intron'])
        k+=2

        # log10 len of CDSparts of best chain matching CDS in tx / log len of CDSparts in chain
        # number of CDS in tx that have any supported by CDSparts from best chain
        if self.best_chain:
            total_length_chain = sum([c[1]-c[0]+1 \
                   for c in evi.group_chains[self.best_chain]['CDSpart']])
            chain_match = sum([sum([c[1]-c[0] + 1 \
                    for c in coords]) for coords in self.cdspart_hints[self.best_chain] if coords])
            
            self.feature_vector[k] = chain_match / total_length_chain
            self.feature_vector[k+1] = np.sum([1 for c in
                evi.group_chains[self.best_chain]['CDSpart'] if c])
        else:
            self.feature_vector[k] = np.log10(self.epsi)
        k+=2

        # list of best_chain support for each CDS
        # list of relativ support by any chain
        # list of absolute support by any chain
        if self.best_chain:
            cds_support = [self.cdspart_hints[self.best_chain], [], []]
        #else:
            #cds_support = [[[]]*len(self.coords['CDS']), [], []]

            for i in range(len(self.coords['CDS'])):
                cds_support[1].append([])
                cds_support[2].append([])
                for group in self.cdspart_hints.values():
                    for c in group[i]:
                        cds_support[2][i].append(c)
                        if cds_support[1][i] and cds_support[1][i][-1][1] >= c[0]:
                            cds_support[1][i][-1][1] = max(cds_support[1][i][-1][1], c[1])
                        else:
                            cds_support[1][i].append(c)


            ### for best chain, relative support by any chain, absolute support by any chain
                # sum support / len(cds)
                # support of maximal supported cds
                # support of minimal supported cds
                # std of support of supported cds
                # entropy of support of supported cds
            for cds in cds_support:
                cds_sum = np.array([np.sum([c2[1] - c2[0] + 1 for c2 in c1]) for c1 in cds]) + self.epsi
                cds_len = np.array([c[1] - c[0] + 1 for c in self.coords['CDS']])
                self.feature_vector[k] = np.sum(cds_sum) / np.sum(cds_len)
                cds_norm = cds_sum / cds_len
                self.feature_vector[k+1] = np.max(cds_norm)
                self.feature_vector[k+2] = np.min(cds_norm)
                self.feature_vector[k+3] = np.std(cds_norm)
                p = (cds_norm / np.sum(cds_norm)) + self.epsi
                if len(self.coords['CDS']) == 1:
                    self.feature_vector[k+4] = 1.0
                else:
                    self.feature_vector[k+4] = - np.sum(p * np.log10(p) / np.log10(len(self.coords['CDS'])))
                k+=5
        
        else:
            k+=15
        # number of overlapping transcripts
        self.feature_vector[k] = len(self.edge_to)
        k+=1
        self.feature_vector[k:] = self.feature_vector[:k]

class Graph_component:
    """
        Connected component of Graph object.
    """
    def __init__(self):
        # list of node IDs in graph component
        self.nodes = []
        # False if new node has been added and edges, incidence_matrix,...
        # haven't been updated
        self.up_to_date = True
        self.incidence_matrix_sender = None
        self.incidence_matrix_receiver = None
        # list Edge() objects of all edges in component
        self.edges = []
        # edges (i,j) in component, where i,j are the indices from self.nodes of adjacent nodes
        self.edge_path = []

    def add_node(self, node):
        self.nodes.append(node.id)
        for e_id in node.edge_to.values():
            if e_id not in self.edges:
                self.edges.append(e_id)
        self.up_to_date = False

    def add_nodes(self, node_list):
        for n in node_list:
            self.add_node(n)

    def update_all(self, edge_dict):
        if not self.up_to_date and self.edges:
            self.__update_edges__(edge_dict)
            self.__update_incidence_matrix__()
        self.up_to_date = True

    def __update_edges__(self, edge_dict):
        if not self.up_to_date:
            self.edge_path = []
            for e_id in self.edges:
                self.edge_path.append([self.nodes.index(edge_dict[e_id].node1),
                    self.nodes.index(edge_dict[e_id].node2)])
                self.edge_path.append([self.nodes.index(edge_dict[e_id].node2),
                    self.nodes.index(edge_dict[e_id].node1)])

    def __update_incidence_matrix__(self):
        n = len(self.nodes)
        m = len(self.edge_path)
        self.incidence_matrix_sender = np.zeros((n,m), bool)
        self.incidence_matrix_receiver = np.zeros((n,m), bool)
        self.incidence_matrix_sender[np.array(self.edge_path)[:,0], \
            range(m)] = True
        self.incidence_matrix_receiver[np.array(self.edge_path)[:,1], \
            range(m)] = True

    def get_incidence_matrix_sender(self):
        if not self.up_to_date:
            self.__update_incidence_matrix__()
        return self.incidence_matrix_sender

    def get_incidence_matrix_receiver(self):
        if not self.up_to_date:
            self.__update_incidence_matrix__()
        return self.incidence_matrix_receiver

    def get_node_features(self, node_dict):
        return np.array([node_dict[n].feature_vector for n in self.nodes])

    def get_edge_features(self, edge_dict):
        edge_features = np.array([[edge_dict[e].feature_vector_n1_to_n2, \
            edge_dict[e].feature_vector_n2_to_n1] for e in self.edges])
        return edge_features.reshape(-1, edge_features.shape[-1])

    def get_target_label(self, node_dict):
        #target[[i for i,n in enumerate(self.nodes) if node_dict[n].is_in_ref_anno], 0] = 1.0
        cds = []
        for n in self.nodes:
            cds += list(node_dict[n].is_in_ref_anno.keys())
            #print(cds)
        cds = set(cds).difference({'is_in_ref_anno'})
        target = np.zeros((len(self.nodes), len(cds)+1), float)
        target[:,0] = np.array([node_dict[n].is_in_ref_anno['is_in_ref_anno'] for n in self.nodes])
        for j, c in enumerate(cds):
            for i, n in enumerate(self.nodes):
                if c in node_dict[n].is_in_ref_anno:
                    target[i,j+1] = 1.0 + node_dict[n].is_in_ref_anno[c]
        
        
        """cds = set([node_dict[n].is_in_ref_anno for n in self.nodes])
        label = [node_dict[n].is_in_ref_anno for n in self.nodes]
        target = np.array([[['', 0.0]]*max([len(l) for l in label])]*len(label))
        #print(target.shape,max([len(l) for l in label]), len(label))
        for i in range(len(label)):
            target[i, :len(label[i])] = label[i]
        target = np.expand_dims(target,-1)"""
        return target

class Graph:
    """
        Overlap (undirected) graph that can detect and filter overlapping transcripts.
    """
    def __init__(self, genome_anno_lst, verbose=0):
        """
            Args:
                genome_anno_lst (list(Anno)): List of Anno class objects
                                              containing genome annotations.
                para (dict(float)): Dictionary for parameter used for filtering of transcripts.
                verbose (int): Verbose mode if verbose >0 .
        """
        # self.nodes['anno;txid'] = Node(anno, txid)
        self.nodes = {}
        # self.edges['ei'] = Edge()
        self.edges = {}

        # self.anno[annoid] = Anno()
        self.anno = {}

        # list of connected graph components
        self.component_list = {}

        # subset of all transcripts that weren't removed by the transcript comparison rule
        self.decided_graph = []

        # dict of duplicate genome annotation ids to new ids
        self.duplicates = {}

        # variables for verbose mode
        self.v = verbose
        self.ties = 0
        self.rng = np.random.RandomState(5253242)


        # list of Graph_component(), each component concists of a number of
        # connected component, all nodes and edges of one connected component is
        # always in the same batch and one connected component is only in one batch
        self.batches = []
        self.batches_no_edge = []
        self.__features_to_norm__ = np.concatenate((np.arange(16) , \
            np.arange(23,27), np.arange(31,35), np.arange(39, 43)))

        # numb nodes
        # numb edges
        # numb components
        self.global_bias_features = np.zeros(30, float)


        # init annotations, check for duplicate ids
        self.init_anno(genome_anno_lst)

    def init_anno(self, genome_anno_lst):
        # make sure that the genome_anno ids are unique
        counter = 0
        for ga in genome_anno_lst:
            if ga.id in self.anno.keys():
                counter += 1
                new_id = "duplicate.anno.{}".format(counter)
                self.duplicates.update({new_id : ga.id})
                ga.change_id(new_id)
            self.anno.update({ga.id : ga})

    def __tx_from_key__(self, key):
        """
            Gets a transcript of a node.

            Args:
                key (str): ID of a node as 'anno_id;tx_id'

            Returns:
                (Transcript): Transcript class object with id = tx_id
                              from Anno() with id = anno_id
        """
        anno_id, tx_id = key.split(';')
        return self.anno[anno_id].transcripts[tx_id]

    def build(self):
        """
            Builds the overlap graph for >=1 Anno() objects.
            Each node of the graph represents a unique transcript from any annotation.
            Two nodes have an edge if their transcripts overlap.
            Two transcripts overlap if they share at least 3 adjacent protein coding nucleotides.
        """
        # tx_start_end[chr] = [tx_id, coord, id for start or end]
        # for every tx one element for start and one for end
        # this dict is used to check for overlapping transcripts
        tx_start_end = {}
        # check for duplicate txs, list of ['start_end_strand']
        unique_tx_keys = {}
        numb_dup = {}
        for k in self.anno.keys():
            for tx in self.anno[k].get_transcript_list():
                if tx.chr not in tx_start_end.keys():
                    tx_start_end.update({tx.chr : []})
                    unique_tx_keys.update({tx.chr : {}})
                unique_key = f"{tx.start}_{tx.end}_{tx.strand}"
                dup = {tx.source_anno}
                if unique_key in unique_tx_keys[tx.chr].keys():
                    check = False
                    coords = tx.get_type_coords('CDS')
                    for t in unique_tx_keys[tx.chr][unique_key]:
                        if coords == t.get_type_coords('CDS'):
                            if tx.utr and not t.utr:
                                unique_tx_keys[tx.chr][unique_key].remove(t)
                                dup.add(t.source_anno)
                            elif tx.utr and t.utr and tx.utr_len  > t.utr_len:
                                unique_tx_keys[tx.chr][unique_key].remove(t)
                                dup.add(t.source_anno)
                            else:
                                numb_dup[f"{t.source_anno};{t.id}"].add(tx.source_anno)
                                check = True
                                break
                    if check:
                            continue
                else:
                    unique_tx_keys[tx.chr].update({unique_key : []})
                numb_dup.update({f"{tx.source_anno};{tx.id}" : dup})
                unique_tx_keys[tx.chr][unique_key].append(tx)

        for chr in unique_tx_keys.keys():
            for tx_list in unique_tx_keys[chr].values():
                for tx in tx_list:
                    key = f"{tx.source_anno};{tx.id}"
                    self.nodes.update({key : Node(tx.source_anno, \
                        tx.id)})
                    self.nodes[key].dup_sources = numb_dup[key]
                    tx_start_end[tx.chr].append([key, tx.start, 0])
                    tx_start_end[tx.chr].append([key, tx.end, 1])

        # detect overlapping nodes
        edge_count = 0
        for chr in tx_start_end.keys():
            tx_start_end[chr] = sorted(tx_start_end[chr], key=lambda t:(t[1], t[2]))
            open_intervals = []
            for interval in tx_start_end[chr]:
                if interval[2] == 0:
                    open_intervals.append(interval[0])
                else:
                    open_intervals.remove(interval[0])
                    for match in open_intervals:
                        tx1 = self.__tx_from_key__(interval[0])
                        tx2 = self.__tx_from_key__(match)
                        if self.__compare_tx_cds__(tx1, tx2):
                            new_edge_key = f"e{edge_count}"
                            edge_count += 1
                            self.edges.update({new_edge_key : Edge(interval[0], match)})
                            self.nodes[interval[0]].edge_to.update({match : new_edge_key})
                            self.nodes[match].edge_to.update({interval[0] : new_edge_key})

    def __compare_tx_cds__(self, tx1, tx2):
        """
            Check if two transcripts share at least 3 adjacent protein
            coding nucleotides on the same strand and reading frame.

            Args:
                tx1 (Transcript): Transcript class object of first transcript
                tx2 (Transcript): Transcript class object of second transcript

            Returns:
                (boolean): TRUE if they overlap and FALSE otherwise
        """
        if not tx1.strand == tx2.strand:
            return False
        tx1_coords = tx1.get_type_coords('CDS')
        tx2_coords = tx2.get_type_coords('CDS')
        for phase in ['0', '1', '2', '.']:
            coords = []
            coords += tx1_coords[phase]
            coords += tx2_coords[phase]
            coords = sorted(coords, key=lambda c:c[0])
            for i in range(1, len(coords)):
                if coords[i-1][1] - coords[i][0] > 1:
                    return True
        return False

    def print_nodes(self):
        # prints all nodes of the graph (only used for development)
        for k in self.nodes.keys():
            print(self.nodes[k].id)
            print(self.nodes[k].transcript_id)
            print(self.nodes[k].anno_id)
            print(self.nodes[k].edge_to.keys())
            print('\n')

    def __find_component__(self, node_id, comp_id):
        self.nodes[node_id].component_id = comp_id
        for next_node_id in self.nodes[node_id].edge_to:
            if not self.nodes[next_node_id].component_id:
                self.__find_component__(next_node_id, comp_id)

    def connected_components(self):
        """
            Compute all clusters of connected transcripts.
            A cluster is connected component of the graph.
            Adds component IDs to nodes.

            Returns:
                (list(list(str))): Lists of list of all node IDs of a component.
        """
        if self.component_list:
            return self.component_list

        self.component_list = {}
        component_index = 1
        for key in self.nodes:
            if not self.nodes[key].component_id:
                c_id = f'g_{component_index}'
                self.component_list.update({c_id : []})
                self.__find_component__(key, f'g_{component_index}')
                component_index += 1
            self.component_list[self.nodes[key].component_id].append(key)
        return self.component_list


    def create_batch(self, numb_batches, batch_size, repl=False):

        if not self.component_list:
            self.connected_components()
        """if not repl and numb_batches*batch_size>len(self.nodes):
            raise BatchSizeError('ERROR: numb_batches*batch_size has to be smaller '\
                + f'than number_connected_components. numb_batches={numb_batches} '\
                + f'batch_size={batch_size} number_connected_components={len(self.component_list)}.')"""
        components = [i for i in list(self.component_list.values()) if len(i)>0]
        self.batches = [Graph_component()]

        end = len(components)
        if repl:
            end = numb_batches*batch_size
        #print(len(self.nodes), len(components), end, numb_batches)
        for k, i in enumerate(np.random.choice(len(components), \
            end, replace=repl)):
            self.batches[-1].add_nodes([self.nodes[n] for n in components[i]])
            if len(self.batches[-1].nodes) >=  batch_size:
                if len(self.batches[-1].edges) == 0:
                    print('#### No edges')
                    self.batches.pop()
                else:
                    self.batches[-1].update_all(self.edges)
                if len(self.batches) <= numb_batches:
                    self.batches.append(Graph_component())
                else:
                    break
        self.batches[-1].update_all(self.edges)


    def create_batch_no_edges(self, numb_batches, batch_size, repl=False):

        if not self.component_list:
            self.connected_components()
        """if not repl and numb_batches*batch_size>len(self.nodes):
            raise BatchSizeError('ERROR: numb_batches*batch_size has to be smaller '\
                + f'than number_connected_components. numb_batches={numb_batches} '\
                + f'batch_size={batch_size} number_connected_components={len(self.component_list)}.')"""
        components = [i for i in list(self.component_list.values()) if len(i)==1]
        self.batches_no_edge = [Graph_component()]

        end = len(components)
        if repl:
            end = numb_batches*batch_size
        #print('NOEDGE', len(self.nodes), len(components), end, numb_batches, len(self.rng.choice(len(components), \
            #end, replace=repl)))
        for k, i in enumerate(self.rng.choice(len(components), \
            end, replace=repl)):
            self.batches_no_edge[-1].add_nodes([self.nodes[n] for n in components[i]])
            if len(self.batches_no_edge[-1].nodes) >=  batch_size:
                self.batches_no_edge[-1].update_all(self.edges)
                if len(self.batches_no_edge) <= numb_batches:
                    self.batches_no_edge.append(Graph_component())
                else:
                    print(k, i)
                    break
        self.batches_no_edge[-1].update_all(self.edges)


    def get_batches_as_input_target(self, val_size=0.1):
        # val size as fraction of numb_batches
        input_target_train = []
        input_target_val = []
        input_target_train_no_edge = []
        input_target_val_no_edge = []
        numb_batches = len(self.batches)
        val_indices = np.random.choice(numb_batches, int(val_size*numb_batches), \
            replace=False)
        for i, batch in enumerate(self.batches):
            input_bias = np.ones((len(batch.nodes), self.global_bias_features.shape[0]), float)
            input_bias *= self.global_bias_features
            new_batch = [
                {
                    "input_nodes" : np.expand_dims(batch.get_node_features(self.nodes) ,0),
                    "input_edges" : np.expand_dims(batch.get_edge_features(self.edges) ,0),
                    #"input_bias" : np.expand_dims(input_bias ,0),
                    "incidence_matrix_sender" : tf.expand_dims(batch.get_incidence_matrix_sender() ,0),
                    "incidence_matrix_receiver" : tf.expand_dims(batch.get_incidence_matrix_receiver() ,0)
                },
                {
                    "target_label" : np.expand_dims(batch.get_target_label(self.nodes),0)
                }
            ]
            if i in val_indices:
                input_target_val.append(new_batch)
            else:
                input_target_train.append(new_batch)

        """numb_batches = len(self.batches_no_edge)
        val_indices = self.rng.choice(numb_batches, int(val_size*numb_batches), \
            replace=False)
        for i, batch in enumerate(self.batches_no_edge):
            input_bias = np.ones((len(batch.nodes), self.global_bias_features.shape[0]), float)
            input_bias *= self.global_bias_features
            new_batch = [
                {
                    "input_nodes" : np.expand_dims(batch.get_node_features(self.nodes) ,0),
                },
                {
                    "target_label" : np.expand_dims(batch.get_target_label(self.nodes),0)
                }
            ]
            if i in val_indices:
                input_target_val_no_edge.append(new_batch)
            else:
                input_target_train_no_edge.append(new_batch)"""
        return input_target_train, input_target_val, input_target_train_no_edge, input_target_val_no_edge

    def add_reference_anno_label(self, ref_anno):
        """
            Sets the value of is_in_ref_anno for each node to 1
            if the coding sequence of the corresponding transcript matches the
            coding sequence of a transcript in the reference anno

            Args:
                ref_anno (Anno): Anno() obeject of reference annotation
        """
        def get_cds_keys(tx):
            keys = [tx.chr, tx.strand] + [str(c[0]) + '_' + str(c[1]) \
                for c in tx.get_type_coords('CDS', frame=False)]
            return keys
        ref_anno_keys = []
        ref_anno_cds = []
        for tx in ref_anno.transcripts.values():
            cds_keys = get_cds_keys(tx)
            ref_anno_cds += cds_keys
            ref_anno_keys.append('_'.join(cds_keys))
        ref_anno_cds = set(ref_anno_cds)
        ref_anno_keys = set(ref_anno_keys)

        for n in self.nodes:
            self.nodes[n].is_in_ref_anno = {'is_in_ref_anno': 0.0}
            c_keys = get_cds_keys(self.__tx_from_key__(n))
            if '_'.join(c_keys) in ref_anno_keys:
                self.nodes[n].is_in_ref_anno['is_in_ref_anno'] = 1.0
            matching_cds = set(c_keys).intersection(ref_anno_cds)
            self.nodes[n].ref_anno_cds_acc = len(matching_cds) / len(c_keys)
            for c in c_keys[2:]:
                self.nodes[n].is_in_ref_anno.update({c : 0.0})
                if c in matching_cds:
                    self.nodes[n].is_in_ref_anno[c] = 1.0
            
            
            if self.nodes[n].ref_anno_cds_acc == 1.0 and self.nodes[n].is_in_ref_anno['is_in_ref_anno'] == 0:
                self.nodes[n].ref_anno_cds_acc = 0.99999999

    def add_node_features(self, evi):
        """
            Compute for all nodes the feature vector based on the evidence support by evi.

            Args:
                evi (Evidence): Evidence class object with all hints from any source.
        """
        #mean = sigma = np.zeros(46, float)

        #ma = np.zeros(self.__features_to_norm__.shape[-1], float)
        #ma = np.zeros(46, float)
        epsi = 1e-17
        n_f = []
        for node_key in self.nodes.keys():
            #def add_node_f:
            tx = self.__tx_from_key__(node_key)
            self.nodes[node_key].add_evidence(tx, evi)
            self.nodes[node_key].add_features(tx, evi)
            #mean += self.nodes[node_key].feature_vector
            n_f.append(self.nodes[node_key].feature_vector)
            #ma = np.max(np.array([ma, \
                #self.nodes[node_key].feature_vector[self.__features_to_norm__]]), 0)
            #ma = np.max(np.array([ma, \
                #self.nodes[node_key].feature_vector]), 0)
        #mean /= len(self.nodes)
        #ma/=2
        #ma += epsi
        n_f = np.array(n_f)
        m = np.max(n_f, axis=0)
        m = np.maximum(m, np.ones(m.shape[0]) * epsi)
        #n_f /= m
        std = np.std(n_f, axis=0)
        std = np.maximum(std, np.ones(std.shape[0]) * epsi)
        mean = np.mean(n_f, axis=0)
        self.global_bias_features = mean
        #print('NODES\nMEAN:  ', mean, '\nSTD:  ', std,
                  #'\nMIN:  ', np.min(n_f, axis=0), '\nMAX:  ',
                  #np.max(n_f, axis=0))
        mean_train = node_features_train_altseqs['mean']
        std_train = np.maximum(node_features_train_altseqs['std'], np.ones(node_features_train_altseqs['std'].shape[0]) * epsi)
        for node_key in self.nodes.keys():
            #print(self.nodes[node_key].feature_vector)
            #self.nodes[node_key].feature_vector[:80] /= m
            self.nodes[node_key].feature_vector[:80] -= mean[:80]
            self.nodes[node_key].feature_vector[:80] /= std[:80]
            #print(self.nodes[node_key].feature_vector)
            """self.nodes[node_key].feature_vector[80:] -= mean_train
            self.nodes[node_key].feature_vector[80:] /= std_train"""
            #print(self.nodes[node_key].feature_vector, '\n\n')

    def add_edge_features(self, evi):
        """
            Compute for all edges the feature vector based on the evidence support by evi.
        """
        epsi = 1e-17
        e_f1 = []
        e_f2 = []
        for edge in self.edges.values():
            #def add_node_f:
            tx1 = self.__tx_from_key__(edge.node1)
            tx2 = self.__tx_from_key__(edge.node2)
            edge.add_features(tx1, tx2, self.nodes[edge.node1], self.nodes[edge.node2], evi)
            e_f1.append(edge.feature_vector_n1_to_n2)
            e_f2.append(edge.feature_vector_n2_to_n1)

        e_f1 = np.array(e_f1)
        m_ni_nj = np.max(e_f1, axis=0)
        m_ni_nj = np.maximum(m_ni_nj, np.ones(m_ni_nj.shape[0]) * epsi)
        #e_f /= m
        std_ni_nj = np.std(e_f1, axis=0)
        std_ni_nj = np.maximum(std_ni_nj, np.ones(std_ni_nj.shape[0]) * epsi)
        mean_ni_nj = np.mean(e_f1, axis=0)
        
        e_f2 = np.array(e_f2)
        m_nj_ni = np.max(e_f2, axis=0)
        m_nj_ni = np.maximum(m_nj_ni, np.ones(m_nj_ni.shape[0]) * epsi)
        #e_f /= m
        std_nj_ni = np.std(e_f2, axis=0)
        std_nj_ni = np.maximum(std_nj_ni, np.ones(std_nj_ni.shape[0]) * epsi)
        mean_nj_ni = np.mean(e_f2, axis=0)

        #print('EDGES\nMEAN:  ', mean, '\nSTD:  ', std,
                  #'\nMIN:  ', np.min(e_f, axis=0), '\nMAX:  ',
                  #np.max(e_f, axis=0))
        mean_train_ni_nj = edge_features_train_ni_nj_altseqs['mean']
        std_train_ni_nj = np.maximum(edge_features_train_ni_nj_altseqs['std'], np.ones(edge_features_train_ni_nj_altseqs['std'].shape[0]) * epsi)
        mean_train_nj_ni = edge_features_train_nj_ni_altseqs['mean']
        std_train_nj_ni = np.maximum(edge_features_train_nj_ni_altseqs['std'], np.ones(edge_features_train_nj_ni_altseqs['std'].shape[0]) * epsi)
        for edge_key in self.edges.keys():
            #print(self.nodes[node_key].feature_vector)
            #self.edges[edge_key].feature_vector_n1_to_n2 /= m
            #self.edges[edge_key].feature_vector_n2_to_n1 /= m
            self.edges[edge_key].feature_vector_n1_to_n2[:120] -= mean_ni_nj[:120]
            self.edges[edge_key].feature_vector_n1_to_n2[:120] /= std_ni_nj[:120]
            self.edges[edge_key].feature_vector_n2_to_n1[:120] -= mean_nj_ni[:120]
            self.edges[edge_key].feature_vector_n2_to_n1[:120] /= std_nj_ni[:120]
            self.edges[edge_key].feature_vector_n1_to_n2[120:] -= mean_train_ni_nj
            self.edges[edge_key].feature_vector_n1_to_n2[120:] /= std_train_ni_nj
            self.edges[edge_key].feature_vector_n2_to_n1[120:] -= mean_train_nj_ni
            self.edges[edge_key].feature_vector_n2_to_n1[120:] /= std_train_nj_ni

    def __decide_edge__(self, edge):
        """
            Apply transcript comparison rule to two overlapping transcripts

            Args:
                edge (Edge): edge between two transcripts

            Returns:
                (str): node ID of the transcript that is marked for removal
        """
        n1 = self.nodes[edge.node1]
        n2 = self.nodes[edge.node2]
        for i in range(0,4):
            diff = n1.feature_vector[i] - n2.feature_vector[i]
            if diff > 0:
                return n2.id
            elif diff < 0:
                return n1.id
        return None

    def decide_component(self, component):
        """
            Applies transcript comparison rule to all transcripts of one component
            and returns the node IDs of all transcripts that are not removed by
            a comparison.

            Args:
                component (list(str)): List of node IDs

            Returns:
                (list(str)): Filtered subset of component list.
        """
        # return all ids of vertices of a graph component, that weren't excluded by the decision rule
        result = component.copy()
        for node_id in component:
            for e_id in self.nodes[node_id].edge_to.values():
                node_to_remove = self.edges[e_id].node_to_remove
                if node_to_remove:
                    if node_to_remove in result:
                        result.remove(node_to_remove)
        return result

    def decide_graph(self):
        """
            Create list of connected components of the graph and apply the
            transcript comparison rule to all components.
        """
        for key in self.edges.keys():
            self.edges[key].node_to_remove = self.__decide_edge__(self.edges[key])
        self.decided_graph = []
        if not self.component_list:
            self.connected_components()
        for component in self.component_list.values():
            if len(component) > 1:
                self.decided_graph += self.decide_component(component)
            else:
                self.decided_graph += component

    def get_decided_graph(self):
        """
            Filter graph with the transcript comparison rule.
            Then, remove all transcripts with low evidence support and
            compute the subset of transcripts that are included in the
            combined gene prediciton.

            Returns:
                (dict(list(list(str))): Dictionary with transcript IDs and new
                gene IDs of all transcripts included in the combined gene prediciton
                for all input annotations

        """
        if not self.decided_graph:
            self.decide_graph()
        # result[anno_id] = [[tx_ids, new_gene_id]]
        result = {}
        for key in self.anno.keys():
            result.update({key : []})
        for node in self.decided_graph:
            if self.nodes[node].evi_support:
                anno_id, tx_id = node.split(';')
                result[anno_id].append([tx_id, self.nodes[node].component_id])
        return result
